
@article{mullachery_bayesian_2018,
	title = {Bayesian {Neural} {Networks}},
	url = {http://arxiv.org/abs/1801.07710},
	abstract = {This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems},
	urldate = {2019-11-11},
	journal = {arXiv:1801.07710 [cs, stat]},
	author = {Mullachery, Vikram and Khera, Aniruddh and Husain, Amir},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.07710},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2019-11-11},
	journal = {arXiv:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = jan,
	year = {2019},
	note = {arXiv: 1806.07366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans ﬁnd visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classiﬁcation on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as conﬁrmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	language = {en},
	urldate = {2019-11-06},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2019-11-06},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{noauthor_gan_nodate,
	title = {{GAN} video generator - {Google} {Search}},
	url = {https://www.google.com/search?sxsrf=ACYBGNQ6sDZaGRtbyBPDul2PIW68iZo3Xw%3A1573037922139&ei=YqfCXbCKCPyk1fAPrLedmA0&q=GAN+video+generator&oq=GAN+video+generator&gs_l=psy-ab.3..0i22i30j0i8i13i30l2.5152.7980..8069...1.3..0.132.2096.0j18......0....1..gws-wiz.......0i71j0j0i22i10i30j35i39j0i10j0i131i20i263j0i67j0i20i263.uBZRdexTxpU&ved=0ahUKEwiww8T0ttXlAhV8UhUIHaxbB9MQ4dUDCAs&uact=5},
	urldate = {2019-11-06}
}

@article{mukherjee_clustergan_2019,
	title = {{ClusterGAN} : {Latent} {Space} {Clustering} in {Generative} {Adversarial} {Networks}},
	shorttitle = {{ClusterGAN}},
	url = {http://arxiv.org/abs/1809.03627},
	abstract = {Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the GAN latent space. In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We compare our results with various clustering baselines and demonstrate superior performance on both synthetic and real datasets.},
	urldate = {2019-10-28},
	journal = {arXiv:1809.03627 [cs, stat]},
	author = {Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.03627},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{liang_dual_2017,
	title = {Dual {Motion} {GAN} for {Future}-{Flow} {Embedded} {Video} {Prediction}},
	url = {http://arxiv.org/abs/1708.00284},
	abstract = {Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.},
	urldate = {2019-10-28},
	journal = {arXiv:1708.00284 [cs]},
	author = {Liang, Xiaodan and Lee, Lisa and Dai, Wei and Xing, Eric P.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.00284},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{saemundsson_variational_2019,
	title = {Variational {Integrator} {Networks} for {Physically} {Meaningful} {Embeddings}},
	url = {http://arxiv.org/abs/1910.09349},
	abstract = {Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose variational integrator networks, a class of neural network architectures designed to ensure faithful representations of the dynamics under study. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.},
	urldate = {2019-10-28},
	journal = {arXiv:1910.09349 [cs, stat]},
	author = {Saemundsson, Steindor and Terenin, Alexander and Hofmann, Katja and Deisenroth, Marc Peter},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09349},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{kwon_predicting_nodate,
	title = {Predicting {Future} {Frames} {Using} {Retrospective} {Cycle} {GAN}},
	abstract = {Recent advances in deep learning have signiﬁcantly improved the performance of video prediction, however, topperforming algorithms start to generate blurry predictions as they attempt to predict farther future frames. In this paper, we propose a uniﬁed generative adversarial network for predicting accurate and temporally consistent future frames over time, even in a challenging environment. The key idea is to train a single generator that can predict both future and past frames while enforcing the consistency of bi-directional prediction using the retrospective cycle constraints. Moreover, we employ two discriminators not only to identify fake frames but also to distinguish fake contained image sequences from the real sequence. The latter discriminator, the sequence discriminator, plays a crucial role in predicting temporally consistent future frames. We experimentally verify the proposed framework using various realworld videos captured by car-mounted cameras, surveillance cameras, and arbitrary devices with state-of-the-art methods.},
	language = {en},
	author = {Kwon, Yong-Hoon and Park, Min-Gyu},
	pages = {10}
}

@article{iten_discovering_2018,
	title = {Discovering physical concepts with neural networks},
	url = {http://arxiv.org/abs/1807.10300},
	abstract = {We introduce a neural network architecture that models the physical reasoning process and that can be used to extract simple physical concepts from experimental data without being provided with additional prior knowledge. We apply the neural network to a variety of simple physical examples in classical and quantum mechanics, like damped pendulums, two-particle collisions, and qubits. The network finds the physically relevant parameters, exploits conservation laws to make predictions, and can be used to gain conceptual insights. For example, given a time series of the positions of the Sun and Mars as observed from Earth, the network discovers the heliocentric model of the solar system - that is, it encodes the data into the angles of the two planets as seen from the Sun. Our work provides a first step towards answering the question whether the traditional ways by which physicists model nature naturally arise from the experimental data without any mathematical and physical pre-knowledge, or if there are alternative elegant formalisms, which may solve some of the fundamental conceptual problems in modern physics, such as the measurement problem in quantum mechanics.},
	urldate = {2019-10-23},
	journal = {arXiv:1807.10300 [physics, physics:quant-ph]},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics}
}

@misc{noauthor_data-driven_nodate,
	title = {Data-driven {Discovery} of {Governing} {Physical} {Laws}},
	url = {https://sinews.siam.org/Details-Page/data-driven-discovery-of-governing-physical-laws},
	abstract = {By Steven L. Brunton, J. Nathan Kutz, and Joshua L. Proctor
Ordinary and partial differential equations are widely used throughout the engineering, physical, and biological sciences to describe the physical laws underlying a given system of interest. We implicitly assume that the governing equations are known and justified by first principles, such as conservation of mass or momentum and/or empirical observations. From the Schrödinger equation of quantum mechanics to Maxwell’s equations for...},
	language = {en-US},
	urldate = {2019-10-23},
	journal = {SIAM News}
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/113/15/3932},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	language = {en},
	number = {15},
	urldate = {2019-10-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pmid = {27035946},
	keywords = {dynamical systems, machine learning, optimization, sparse regression, system identification},
	pages = {3932--3937}
}

@article{iten_discovering_2018-1,
	title = {Discovering physical concepts with neural networks},
	url = {http://arxiv.org/abs/1807.10300},
	abstract = {We introduce a neural network architecture that models the physical reasoning process and that can be used to extract simple physical concepts from experimental data without being provided with additional prior knowledge. We apply the neural network to a variety of simple physical examples in classical and quantum mechanics, like damped pendulums, two-particle collisions, and qubits. The network finds the physically relevant parameters, exploits conservation laws to make predictions, and can be used to gain conceptual insights. For example, given a time series of the positions of the Sun and Mars as observed from Earth, the network discovers the heliocentric model of the solar system - that is, it encodes the data into the angles of the two planets as seen from the Sun. Our work provides a first step towards answering the question whether the traditional ways by which physicists model nature naturally arise from the experimental data without any mathematical and physical pre-knowledge, or if there are alternative elegant formalisms, which may solve some of the fundamental conceptual problems in modern physics, such as the measurement problem in quantum mechanics.},
	urldate = {2019-10-23},
	journal = {arXiv:1807.10300 [physics, physics:quant-ph]},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics}
}

@article{de_silva_discovery_2019,
	title = {Discovery of {Physics} from {Data}: {Universal} {Laws} and {Discrepancy} {Models}},
	shorttitle = {Discovery of {Physics} from {Data}},
	url = {http://arxiv.org/abs/1906.07906},
	abstract = {Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of subtle and nuanced issues that must be addressed by modern data-driven methods for the automated discovery of physics. Specifically, we show that measurement noise and complex secondary physical mechanisms, such as unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. Without proposing an appropriate discrepancy model to handle these drag forces, the data supports an Aristotelian, versus a Galilean, theory of gravitation. Using the sparse identification of nonlinear dynamics (SINDy) algorithm, with the additional assumption that each separate falling object is governed by the same physical law, we are able to identify a viable discrepancy model to account for the fluid dynamic forces that explain the mismatch between a posited universal law of gravity and the measurement data. This work highlights the fact that the simple application of ML/AI will generally be insufficient to extract universal physical laws without further modification.},
	urldate = {2019-10-23},
	journal = {arXiv:1906.07906 [physics, stat]},
	author = {de Silva, Brian and Higdon, David M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07906},
	keywords = {Computer Science - Machine Learning, Physics - Classical Physics, Statistics - Machine Learning}
}

@article{greydanus_hamiltonian_2019,
	title = {Hamiltonian {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.01563},
	abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
	urldate = {2019-10-17},
	journal = {arXiv:1906.01563 [cs]},
	author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.01563},
	keywords = {Computer Science - Neural and Evolutionary Computing}
}

@misc{noauthor_[1906.01563]_nodate,
	title = {[1906.01563] {Hamiltonian} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1906.01563},
	urldate = {2019-10-17}
}

@article{iten_discovering_2018-2,
	title = {Discovering physical concepts with neural networks},
	url = {http://arxiv.org/abs/1807.10300},
	abstract = {We introduce a neural network architecture that models the physical reasoning process and that can be used to extract simple physical concepts from experimental data without being provided with additional prior knowledge. We apply the neural network to a variety of simple physical examples in classical and quantum mechanics, like damped pendulums, two-particle collisions, and qubits. The network finds the physically relevant parameters, exploits conservation laws to make predictions, and can be used to gain conceptual insights. For example, given a time series of the positions of the Sun and Mars as observed from Earth, the network discovers the heliocentric model of the solar system - that is, it encodes the data into the angles of the two planets as seen from the Sun. Our work provides a first step towards answering the question whether the traditional ways by which physicists model nature naturally arise from the experimental data without any mathematical and physical pre-knowledge, or if there are alternative elegant formalisms, which may solve some of the fundamental conceptual problems in modern physics, such as the measurement problem in quantum mechanics.},
	urldate = {2019-10-16},
	journal = {arXiv:1807.10300 [physics, physics:quant-ph]},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics}
}

@misc{noauthor_[1901.01892]_nodate,
	title = {[1901.01892] {Scale}-{Aware} {Trident} {Networks} for {Object} {Detection}},
	url = {https://arxiv.org/abs/1901.01892},
	urldate = {2019-09-18}
}

@misc{noauthor_[1703.06870]_nodate,
	title = {[1703.06870] {Mask} {R}-{CNN}},
	url = {https://arxiv.org/abs/1703.06870},
	urldate = {2019-09-18}
}